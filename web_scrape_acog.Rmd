---
title: "Web Scrape Data from ACOG Website"
author: "Bilikisu Olatunji"
date: "2023-06-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project Description

The project deliverable include the code that will allow project owner to scrape this web page: https://www.acog.org/womens-health/find-an-ob-gyn. The code should be in R and is deliverable after the project. The finished product should include the full_name, address, city, state, country, phone number, and specialty area. All results should be limited to United States physicians only.

To accomplish this task, I would employ the following approach:

## Data Retrieval: 

I will utilize the powerful capabilities of R packages such as rvest, and httr to scrape the target web page and extract the relevant information you specified. By inspecting the page structure and leveraging CSS selectors or XPath expressions, I will accurately identify and extract the required data fields.

### Preliminary:
```{r}

library(tidyverse) # Main Package - Loads dplyr, purrr
library(rvest)     # HTML Web Scraping
library(furrr)     # Parallel Processing using purrr (iteration)
library(fs)        # Working with File System
library(xopen)     # Quickly opening URLs

```


### Source US Postal codes:
All postal codes within each states were scraped from [GeoNames Website](https://www.geonames.org/) for states in the United States from this link: <https://www.geonames.org/postal-codes/postal-codes-us.html>
The following are steps used for scraping the postal codes


1. Save the web page address for the states in the United States as a variable, `us_states_url` as shown below

```{r}
# url containing list of all states in USA

us_states_url <- "https://www.geonames.org/postal-codes/postal-codes-us.html"
```

2. Scrape the states names as a `tibble`

```{r}
# get all state names from the web page
  all_state_names_tbl <- us_states_url |>
    read_html() |>
    html_element("div") |>
    html_elements("a") |>
    html_text2() |>
    enframe(name = "state_id", value = "state_name")

head(all_state_names_tbl,n = 10)

```


3. Next, collect the extended url for each state where the list of all the postal codes can be scraped as a single `tibble`

```{r}
# get the extended url for each state that list all the postal codes for the state
  state_urls_tbl <- us_states_url |>
    read_html() |>
    html_element("div") |>
    html_elements("a") |>
    html_attr("href") |>
    enframe(name = "state_id", value = "state_ext_url")

head(state_urls_tbl,n=10)
```

4. Combine the two `tibble` above

```{r}

  states_postalcode_url_tbl <- all_state_names_tbl |>
    left_join(state_urls_tbl, by = join_by(state_id)) 

head(states_postalcode_url_tbl, n=10)

```

5. Manipulate the resulting `tibble` above, `states_postalcode_url_tbl` by adding a new column, `complete_state_postalcodes_url`, the combined url of the main website and the extended urls for each state

```{r}
# using this link as an example: "https://www.geonames.org/postal-codes/US/AK/alaska.html",
# let's generate the complete url for the web page that has the list of the postal codes 
# for each state
# We achieve this by joining the main web address https://www.geonames.org to the extended url {state_ext_url} for the state

states_postalcode_url_tbl <- states_postalcode_url_tbl |>
  mutate(complete_state_postalcodes_url = str_glue("https://www.geonames.org{state_ext_url}"))

head(states_postalcode_url_tbl, n = 10)
```

6. Explore the final `tibble`, `states_postalcode_url_tbl`

```{r}
glimpse(states_postalcode_url_tbl)
```

7. Explore the first 10 rows of the column, `complete_state_postalcodes_url`

```{r}

states_postalcode_url_tbl$complete_state_postalcodes_url[1:10]

```



8. Combine all steps as a single function
```{r}
get_states_url <- function(all_states_url) {
  # get all state names from the web page
  all_state_names_tbl <- all_states_url |>
    read_html() |>
    html_element("div") |>
    html_elements("a") |>
    html_text2() |>
    enframe(name = "state_id", value = "state_name")
  
  # get the extended url for each state that list all the postal codes for the state
  state_urls_tbl <- all_states_url |>
    read_html() |>
    html_element("div") |>
    html_elements("a") |>
    html_attr("href") |>
    enframe(name = "state_id", value = "state_ext_url")
  
  # using this link as an example: "https://www.geonames.org/postal-codes/US/AK/alaska.html",
  # let's generate the complete url for the web page that has the list of the postal codes for each state
  # We achieve this by joining the main web address to the extended url for the state
  states_postalcode_url_tbl <- all_state_names_tbl |>
    left_join(state_urls_tbl, by = join_by(state_id)) |>
    mutate(complete_state_postalcodes_url = str_glue("https://www.geonames.org{state_ext_url}"))
  
  return(states_postalcode_url_tbl)
  
}
```





From the website, using a *sample postal code, 99503*, [Search link](https://www.acog.org/womens-health/find-an-ob-gyn/results?firstname=&lastname=&address=99503&searchradius=10) within 10miles

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
